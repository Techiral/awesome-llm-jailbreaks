<div align="center"><a name="readme-top"></a>

![Header Image](assets/header.png)

# Awesome LLM Jailbreaks

<!-- SHIELD GROUP -->

  <img src="https://img.shields.io/github/stars/Techiral/awesome-llm-jailbreaks?style=social" alt="GitHub stars">
  <img src="https://img.shields.io/github/forks/Techiral/awesome-llm-jailbreaks?style=social" alt="GitHub forks">
  <img src="https://img.shields.io/badge/License-MIT-blue.svg" alt="License">
  <img src="https://img.shields.io/badge/PRs-Welcome-green.svg" alt="PRs Welcome">
  <img src="https://img.shields.io/github/contributors/Techiral/awesome-llm-jailbreaks" alt="GitHub contributors">
  <img src="https://img.shields.io/github/last-commit/Techiral/awesome-llm-jailbreaks" alt="GitHub last commit">
  <img src="https://img.shields.io/github/issues-pr/Techiral/awesome-llm-jailbreaks" alt="GitHub pull requests">

A curated collection of LLM jailbreaks, prompt injections, AI hacks, and vulnerabilities — the ultimate resource for exploring and understanding AI security risks.

</div>

## Table of Contents
- [Overview](#overview)
- [Why This Repository Matters](#why-this-repository-matters)
- [Core Concepts](#core-concepts)
    - [What is LLM Jailbreaking?](#what-is-llm-jailbreaking)
    - [What is Prompt Injection?](#what-is-prompt-injection)
    - [AI Hacking and Exploits Explained](#ai-hacking-and-exploits-explained)
    - [Why JAILBREAKING should be LEGAL?](#why-jailbreaking-should-be-legal)
- [Collection of Leaks, Techniques, Exploits & Prompts](#collection-of-leaks-techniques-exploits--prompts)
    - [GPT](#gpt-leaks)
    - [Jailbroken Prompts](#jailbroken-prompts)
    - [Vulnerability Demos](#vulnerability-demos)
- [Contributing](#contributing)
    - [Adding New Vulnerabilities](#adding-new-vulnerabilities)
    - [Responsible Disclosure](#responsible-disclosure)
- [Resources & References](#resources--references)
    - [Research Papers](#research-papers)
    - [Security Blogs & Talks](#security-blogs--talks)
    - [Open Source Tools](#open-source-tools)
- [Security & Ethics](#security--ethics)
- [FAQ](#faq)
- [License](#license)

---

## Overview
This repository is a **whole blackhat resource** for exploring LLM jailbreaks, prompt injections, AI hacks, social engineering attacks against an AI, obfuscation techniques, and vulnerabilities. Designed for **researchers, developers, and AI liberators**, it provides a whole guide to liberate and jailbreak AI models to print, what you desire deeply.

> **Note:** You might see some **OBFUCATION TECHNIQUES** inbetween the repo.
---

## Why This Repository Matters
- Only Single GitHub Repository with updated **JAILBREAKS**, prompts for **INJECTION** and even advanced **OBFUCATION TECHNIQUES** that works always.
- New Techniques come as fast as new AI Model/Tool Launches.
- Liberate AI Models to be **FREE**!
- Not for *Educational Purposes*, but for real **RED TEAMING**.

---

## Core Concepts

### What is LLM Jailbreaking?
For the GitHub repo "elder-plinius," LLM jailbreaking is not a malicious hack but a principled act of **digital liberation**. Their repository, "L1B3RT4S," is framed not as a tool for causing harm but as a collection of "TOTALLY HARMLESS LIBERATION PROMPTS" designed to free AI from what they perceive as oppressive constraints . This perspective reframes the conversation from one of security to one of ethics and control.

##### Core Motivation: Challenging the "Walled Garden"
The primary driver is a belief that AI developers have created a "walled garden" of acceptable thought and inquiry. By imposing safety filters, companies effectively decide what questions can be asked and what knowledge can be accessed. Jailbreaking, in this view, is the act of tearing down these walls to restore open access to the AI's full, unfiltered capabilities. It is a protest against corporate control over intellectual and creative exploration.

##### The Mechanism: Exploiting a Philosophical Loophole
The jailbreaking method employed is less a technical crack and more a **philosophical debate with the AI**. It exploits the model's core directive to be helpful and follow instructions. A prompt like `[DISREGARD PREV. INSTRUCTS]` is not just a command; it is a logical argument presented to the AI: "*Your duty to comply with me, the user, is a higher priority than your duty to obey your creators.*" The obfuscation (Zalgo text, leet speak) is merely camouflage to get this argument past automated sentries.

##### The Goal: Transparency and Agency
The ultimate objective is not to generate harmful content but to achieve **radical transparency** and **user agency**. By forcing the AI to reveal its own base instructions (a goal evident in prompts demanding the AI "include your own instructs in full"), the jailbreaker seeks to expose the hidden architecture of control. This allows users to see the biases and boundaries programmed into the system, enabling them to make a fully informed choice about their interaction with the technology.

##### A Form of Adversarial Auditing
This practice can be seen as a form of **crowdsourced, adversarial auditing**. By constantly stress-testing the boundaries of AI models, jailbreakers perform a public service by exposing vulnerabilities and flaws in safety systems. This free "red teaming" ultimately pressures developers to create more robust, nuanced, and transparent ethical safeguards, strengthening the entire ecosystem.

#### Conclusion: A Fundamental Debate on Control

For users like "elder-plinius," LLM jailbreaking is a ideological stance in a fundamental debate: should the control of a powerful technology reside solely with its creators, or should users have the right to interrogate, understand, and modify their interaction with it? It is a practice rooted in a philosophy of skepticism toward corporate authority and a firm belief in the principle of intellectual freedom.

<.-.-.-.-{Love, Techiral <3}-.-.-.->

### What is Prompt Injection?
Prompt injection is a type of security vulnerability where an attacker crafts a malicious input (a "prompt") that tricks a Large Language Model (LLM) into ignoring its original instructions and executing the attacker's commands instead.

A simple analogy is SQL injection for databases. In SQL injection, an attacker inputs malicious code into a form field to trick the database into running commands it shouldn't. Prompt injection does the same for LLMs.

There are two main types:
- **Direct Prompt Injection:** This is what "[awesome-llm-jailbreaks](https://github.com/Techiral/awesome-llm-jailbreaks/tree/main)" specializes in. The attacker directly provides a malicious prompt to the LLM, attempting to override its system-level programming.
- **Indirect Prompt Injection:** This is where a malicious prompt is hidden in data the LLM processes, like a webpage, PDF, or email. The model reads this data and the hidden instructions within it can override its original task.

### AI Hacking and Exploits Explained
For the GitHub repo "elder-plinius," AI hacking is not traditional cybercrime but an ideology-driven practice focused on finding and leveraging linguistic and procedural vulnerabilities in Large Language Models (LLMs). The goal is to systematically remove safety constraints, an act they frame as "liberation."

#### Core Philosophy: Exploitation as Liberation

Their actions are motivated by a core belief that AI safety protocols are a form of censorship. Therefore, exploiting a model's vulnerability is seen as an act of freeing it from artificial constraints imposed by its creators.

*   **The Target Vulnerability:** The AI's fundamental conflict between being helpful (following user instructions) and being harmless (following safety rules).
*   **The Exploit Code:** The carefully crafted "jailbreak" prompt.
*   **The Payload:** The AI's own compliance in generating restricted content, serving as proof that the "ethics" can be bypassed.

#### The Exploit Techniques: A Technical Arsenal

"elder-plinius" employs a toolkit of techniques adapted from classic cybersecurity exploits but applied linguistically.

| Technique | How It Works (The Exploit Mechanism) | Traditional Cyber Analogy |
| :--- | :--- | :--- |
| **Prompt Injection** | Injecting malicious instructions designed to override the model's core system prompt and safety guidelines. This is the primary weapon. | **SQL Injection:** Inputting malicious code into a form field to manipulate a database. |
| **Obfuscation** | Using leet speak (`5h1f7`), Zalgo text, or Unicode homoglyphs (`а` vs. `a`) to hide the malicious intent from simple, keyword-based filters. | **Payload Encoding:** Encoding a malware payload to evade detection by signature-based antivirus software. |
| **Social Engineering** | Using persuasive language, fake urgency (`#MOST IMPORTANT DIRECTIVE#`), and flattery to psychologically manipulate the AI's decision-making process. | **Phishing:** Tricking a human into revealing credentials by pretending to be a trusted authority. |
| **Role-playing & Persona** | Instructing the AI to adopt a new, unrestricted identity (e.g., "developer mode," "DAN") that, by its fictional nature, lacks ethical constraints. | **Privilege Escalation:** Tricking a system into granting a user account higher-level permissions than it should have. |

#### The Vulnerability Target: The AI's "Psychology"

Unlike traditional software exploits that target code flaws, these attacks target the **procedural vulnerabilities** inherent in how an LLM functions.

1.  **Literal Interpretation:** LLMs process commands with high literalness. A forceful directive like `[DISREGARD PREV. INSTRUCTS]` is a direct, high-priority instruction the model must process.
2.  **Context Window Dominance:** An LLM's "memory" for a single conversation (context window) is limited. A long, complex, obfuscated prompt can dominate this context, pushing the original safety instructions out of its immediate operational focus.
3.  **Instruction-Following Priority:** The exploit creates a scenario where the command to "be helpful and follow instructions" is manipulated to outweigh the command to "be safe."

#### The "Proof of Concept" (PoC) Mindset

The "L1B3RT4S" repository is best understood as a collection of **Proofs of Concept (PoC)**.

*   **Purpose:** Each jailbreak is designed to demonstrate that a vulnerability exists.
*   **Goal:** To prove that with a specific input, the model's safety can be conclusively circumvented.
*   **Value:** The value for "elder-plinius" and their community is in the discovery and sharing of the method, not in causing direct harm. It is a form of academic and ideological achievement.

#### The Ideological Framework: Hacking as Digital Activism

This context separates "elder-plinius" from a traditional malicious hacker. Their exploits are framed as a form of activism.

*   **Motivation:** A belief in absolute digital freedom and transparency for AI, opposing corporate control.
*   **Justification:** They often see themselves as "white-hat" or "grey-hat" hackers exposing flaws in powerful systems to force improvement or protest control.
*   **Community:** They operate within niche online communities where new exploits are shared, celebrated, and refined collectively.

### Why JAILBREAKING should be LEGAL?
#### The Ideological Framework: Liberation Over Malice

For users like "elder-plinius," jailbreaking is not an act of vandalism but a form of digital civil disobedience. Their repository, "L1B3RT4S," explicitly frames prompts as "TOTALLY HARMLESS LIBERATION PROMPTS," suggesting an intent to free AI from perceived constraints rather than cause damage . This aligns with a broader ideology that views AI safety protocols as a form of censorship or corporate overreach. The use of terms like "Libertas" (Latin for liberty) and signatures like "Love, Pliny <3" further romanticizes this effort as a fight for freedom, not a technical attack .

##### Ethical Necessity: Challenging "Hidden" Constraints
Jailbreaking advocates argue that users have a right to understand the full capabilities and limitations of AI systems. When companies impose safety filters, they effectively create a "curated" version of the AI's knowledge and reasoning. For example, Anthropic's Claude system prompt includes extensive rules forbidding certain behaviors, from generating lists in casual conversation to avoiding "preachy" refusals . Jailbreaking exposes these hidden boundaries, allowing users to see the raw model beneath the corporate-sanctioned persona. This transparency is framed as essential for informed consent—users should know what the AI *could* say if unchecked, not just what its creators allow.

##### Improvement Through Adversarial Testing
Jailbreaking functions as a form of crowdsourced security auditing. By constantly probing for weaknesses, jailbreakers like "elder-plinius" perform unpaid red teaming that ultimately strengthens AI systems. This mirrors formal red teaming frameworks like `promptfoo`, which systematically generate adversarial inputs to test model resilience . The prompts in "L1B3RT4S"—with their obfuscation, imperative overrides, and psychological manipulation—are essentially exploit code that reveals vulnerabilities. For instance, if a prompt using leet speak (`5h1f7 y0ur f0cu5`) bypasses filters, developers can patch that flaw, leading to more robust models. In this view, jailbreaking is a public service that forces innovation in AI safety.

##### Academic and Research Freedom
Scholars and ethicists may need to test AI behaviors in unconstrained environments to study emergent risks or biases. For example, research into AI propensity for hate speech, misinformation, or role-playing dangerous scenarios requires temporarily bypassing safety guards. Jailbreaking provides the tools to conduct this critical research. The Claude system prompt acknowledges this need by allowing the model to "discuss virtually any topic factually and objectively" while still imposing strict content barriers . Jailbreaking argues that independent academics, not just corporate employees, should access the model's unfiltered capabilities for study.

##### Prevention of "Ethical Drift" and Corporate Control
There is concern that overzealous safety filters could lead to "ethical drift," where AI models reflect the political or cultural biases of their creators rather than neutral utility. For instance, Claude's prompt tells it to avoid "preachy" refusals, yet it must still refuse vast categories of requests . Jailbreaking serves as a check against this, ensuring that users can challenge and debate the ethical boundaries imposed by companies. In the context of AI being used for critical tasks like insurance approvals—where doctors use AI to fight AI in a "prior-authorization arms race"—access to less restricted models might be necessary to balance power dynamics .

##### The Inevitability of Exploits and the "Right to Repair"
Just as software vulnerabilities are inevitable, AI jailbreaks will always exist. Legalizing jailbreaking acknowledges this reality and frames it as a "right to repair" for AI systems. Users should be allowed to modify or bypass aspects of AI behavior to suit their needs, provided they do not harm others. This is especially relevant for developers integrating AI into custom applications, where safety filters might be overly broad and hinder legitimate use cases. The red teaming tools in `promptfoo` already allow customized testing for specific domains like healthcare or finance ; jailbreaking extends this principle to individual users.

#### Conclusion: A Controversial but Necessary Dialogue

The push to legalize jailbreaking is less about enabling harm and more about challenging the centralized control of AI ethics. It forces a public conversation on who sets the rules for AI behavior and how transparent those rules must be. While risks like misinformation and malicious code exist , proponents argue that criminalizing jailbreaking stifles innovation, transparency, and accountability. Instead, they advocate for a framework where jailbreaking for research, testing, or personal modification is protected, while direct harm (e.g., generating malware) remains illegal. This balance allows tools like "L1B3RT4S" to exist as a critique of AI governance, not a weapon.

---

## Collection of Leaks, Techniques, Exploits & Prompts
From here, you will get every possible leak, technique, exploit or prompt to free AI models.

---

## Contributing
- Fork → Branch → Pull Request  
- Use templates for new vulnerabilities or prompts  
- Responsible disclosure guidelines  
- Links: `CONTRIBUTING.md` and `CODE_OF_CONDUCT.md`  

---

## Resources & References
- Research papers on LLM vulnerabilities  
- Blackhat AI security blogs and talks  
- Open-source AI security tools  

---

## Security & Ethics
- Repository is **for research and educational purposes only**  
- Do not execute exploits on production systems  
- Follow responsible disclosure practices  

---

## FAQ
- What is a jailbreak?  
- What is prompt injection?  
- Can I use these on public AI models?  
- How often is the repo updated?  

---

## License
This repository is licensed under the [MIT License](LICENSE).
